{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYNwBQqCzaIy"
   },
   "source": [
    "# Tobig's 20기 정규세션 1주차 과제(2): Frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVAk7BpV1SCd"
   },
   "source": [
    "- 제출자: 20기 황태연\n",
    "- 제출 일자: 2023.07.25. (화)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EyPML7ul1buV"
   },
   "source": [
    "- **과제**\n",
    "    1. Library와 Framework의 차이를 간단하게 서술하기. (100자 내외)\n",
    "    2. 딥러닝과 머신러닝의 관계 및 특징, 차이를 간단하게 서술하기. (200자 내외)  \n",
    "    3. 2개의 예제 ipynb 코드를 보고 이해한 뒤 주석 달기. (과제에서 제외됨.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xz9IQrxl0_7V"
   },
   "source": [
    "# 1. Library와 Framework의 차이"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rO3ECL3AH6Ui"
   },
   "source": [
    "- **프레임워크**는 구조와 규칙을 제공하는 기반으로, 개발자는 프레임워크가 제시하는 규칙을 따르며 코드를 구현한다.  \n",
    "반면 **라이브러리**는 재사용 가능한 코드의 집합으로, 개발자는 자신의 코드에 필요한 기능들을 라이브러리에서 찾아 활용한다.  \n",
    "(102자)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9HYPd666THrP"
   },
   "source": [
    "(참고)\n",
    "- 프레임워크의 예: django, CherryPy 등\n",
    "- 라이브러리의 예: pandas, Numpy, matplotlib 등\n",
    "- Pytorch와 Tensorflow는 프레임워크와 라이브러리의 구분이 명확하지 않은 것으로 보인다. 특히 위키백과 영문판에서는 'Pytorch는 Torch라는 라이브러리를 기반으로 하는 프레임워크'라고 언급하고, '딥러닝 파이토치 교과서' 에서도 프레임워크라고 명시하고 있다. 그러나 위키백과 한글판에서는 라이브러리라고 언급하고 있으며, '딥러닝 Express' 교재에서도 Tensorflow와 Pytorch를 라이브러리라고 명시하고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7hjqdvEVKFd"
   },
   "source": [
    "# 2. 딥러닝과 머신러닝의 관계와 특징"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EpXA4mDIVKJN"
   },
   "source": [
    "- **머신러닝**은 인공지능의 한 분야로서 스스로 학습하는 프로그램을 연구하는 분야이다. **딥러닝**은 머신러닝의 한 분야로서 인공 신경망 등을 이용하여 빅데이터로부터 스스로 학습하는 프로그램을 연구하는 분야이다. 즉, <u>딥러닝은 머신러닝의 하위 분야</u>이다.\n",
    "- 딥러닝의 가장 큰 특징은 인간의 뇌를 모방한 **인공 신경망**을 활용한다는 점이다. 이를 통해 상당히 복잡한 패턴을 파악하는 것이 가능해졌으며, 이미지, 자연어, 음성 등에서 훌륭한 성과를 내고 있다.  \n",
    "(190자)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zhwJOC4maAid"
   },
   "source": [
    "# 3. 2개의 예제 ipynb 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zaXHRRU7af6P"
   },
   "source": [
    "## 3.1. sklearn 라이브러리를 활용한 Random Forest와 XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXWPAd6zVeMd"
   },
   "source": [
    "- 랜덤 포레스트(Random Forest)의 기반이 되는 머신러닝 알고리즘은 결정 트리(Decision Tree)이다. 3.1.1절에서는 결정 트리가 어떤 알고리즘인지 살펴본다.\n",
    "- 랜덤 포레스트는 결정 트리의 앙상블이라고 볼 수 있는데, 이때 앙상블이 무엇인지는 3.1.2절에서 살펴본다.\n",
    "- 3.1.3절에서는 랜덤 포레스트가 무엇인지 살펴보고, 앙상블 방법 중 하나인 부스팅(Boosting)을 이용한 XGBoost에 대해 간단히 소개한다.\n",
    "- 해당 내용은 '핸즈온 머신러닝 2판' 내용을 바탕으로 작성하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKqhZ97FUnI8"
   },
   "source": [
    "### 3.1.1. 결정 트리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dcKYDvQUrJ0"
   },
   "source": [
    "- **결정 트리(decision tree)**: 트리 구조를 사용하여 분류와 회귀 작업을 수행하는 머신러닝 알고리즘\n",
    "- 결정 트리를 시각화하면 다음과 같다.\n",
    "\n",
    "<img src=\"01.png\" width=50% height=50%>  \n",
    "\n",
    "(타이타닉호 탑승객의 생존 여부를 나타내는 결정 트리. | sibsp: 탑승한 배우자와 자녀의 수 | 리프 노드 아래: (생존 확률, 탑승객의 비율))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgIi2K5aZAST"
   },
   "source": [
    "- **루트 노드(root node)**: 깊이가 0인 맨 꼭대기의 노드\n",
    "- **리프 노드(leaf node)**: 자식 노드(child node)를 가지지 않는 노드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xh1xExMXb3oz"
   },
   "source": [
    "#### 3.1.1.1. 결정 트리의 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJEnrbBtbjFd"
   },
   "source": [
    "- 결정 트리는 어떻게 학습을 할 수 있을까?\n",
    "- **불순도(impurity)**: 분류된 노드에 속하는 샘플들의 혼잡도\n",
    "    - 보통 다음과 같은 **지니(gini) 불순도**를 사용한다.\n",
    "$$\n",
    "G_{i}=1-\\sum_{k=1}^{n} p_{i, k}^2\n",
    "$$\n",
    "($G_{i}$: $i$번째 노드의 지니 불순도, $p_{i, k}$: $i$번째 노드에 있는 샘플 중 $k$ 클래스에 속한 샘플의 비율)\n",
    "    - 예 1) [A, B] 클래스 분류 작업에서 한 노드에 [0, 50]의 샘플이 속해있다면 다음과 같이 계산된다.\n",
    "$$\n",
    "G= 1-((0/50)^2+(50/50)^2) = 0\n",
    "$$\n",
    "        - 이렇게 모든 샘플이 같은 클래스에 속해 있는 노드를 **순수 노드**라고 하고, 이때의 지니 불순도는 0이다.\n",
    "    - 예 2) [A, B] 클래스 분류 작업에서 한 노드에 [10, 40]의 샘플이 속해있다면 다음과 같이 계산된다.\n",
    "$$\n",
    "G= 1-((10/50)^2+(40/50)^2) = 0.32\n",
    "$$\n",
    "    - 예 3) [A, B] 클래스 분류 작업에서 한 노드에 [20, 30]의 샘플이 속해있다면 다음과 같이 계산된다.\n",
    "$$\n",
    "G= 1-((20/50)^2+(30/50)^2) = 0.48\n",
    "$$\n",
    "        - 샘플의 혼잡도가 높을수록 불순도도 높아지는 것을 확인할 수 있다.\n",
    "    - 다음과 같은 **엔트로피(entroopy) 불순도**가 사용되기도 한다.\n",
    "$$\n",
    "H_{i} = -\\sum_{k=1\\\\p_{i, k} \\neq 0}^{n}p_{i, k}\\log_{2}(p_{i, k})\n",
    "$$\n",
    "        - 마찬가지로 샘플의 혼잡도가 높을수록 불순도가 높아진다.\n",
    "\n",
    "- 결정 트리는 순수 노드가 아닌 노드를 분할하며 학습한다. 이때 각 노드의 불순도가 낮아지는 방향으로 학습한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w67ghjLOgsoY"
   },
   "source": [
    "- **CART(Classification and Regreesion Tree) 훈련 알고리즘**: 결정 트리를 생성하는 알고리즘 중 하나로, 다음과 같은 과정을 반복한다.\n",
    "- 훈련 세트를 하나의 특성 $k$와 임곗값 $t_k$를 사용해 두 개의 서브셋으로 나눈다. 이때 다음 CART 비용 함수가 최소가 되도록 특성과 임곗값을 선택한다.\n",
    "$$\n",
    "J(k, t_k) = \\frac{m_{left}}{m}G_{left} + \\frac{m_{right}}{m}G_{right}\n",
    "$$\n",
    "($G_{left/right}$: 왼쪽/오른쪽 서브셋의 (**분류 문제**)**불순도** 또는 (**회귀 문제**)**평균제곱오차(MSE)**, $m_{left/right}$: 왼쪽/오른쪽 서브셋의 샘플 수)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbcVB9opnN_a"
   },
   "source": [
    "#### 3.1.1.2. 결정 트리의 장점과 단점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULepo2wPnTcX"
   },
   "source": [
    "- 결정 트리의 장점: 1) 이해하고 해석하기 쉽다. 2) 사용하기 편리하다. 3) 여러 용도로 사용할 수 있다. 4) 성능이 뛰어나다.\n",
    "- 결정 트리의 단점: 1) 과적합(overfitting)이 발생할 가능성이 높다. 2) 훈련 데이터의 작은 변화에 매우 민감하다.\n",
    "    - 과적합을 피하기 위해 **규제 매개변수**를 이용한다.\n",
    "    - **규제 매개변수**: 머신 러닝 모델의 하이퍼파라미터 중 하나로, 모델의 학습에 제한을 두는 데에 사용된다.\n",
    "        - 예: 결정 트리에서 트리의 최대 깊이(max_depth), 리프 노드의 최대 수(max_leaf_nodes) 등"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a37H__5noyI7"
   },
   "source": [
    "### 3.1.2. 앙상블"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_l31f6JpJD4"
   },
   "source": [
    "- **앙상블(Ensemble)**: 일련의 모델들을 조합하여 더 좋은 예측을 하는 모델을 만드는 것\n",
    "    - 예: 1) 보팅(Voting), 2) 배깅(Bagging), 3) 부스팅(Boosting), 4) 스태킹(Stacking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-4xT9kPqR7R"
   },
   "source": [
    "#### 3.1.2.1. 보팅(Voting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWmij5rKpdAD"
   },
   "source": [
    "- **보팅(Voting)**: 투표 방식을 통해 여러 분류기의 예측을 종합하는 방식\n",
    "    - **직접 투표(Hard Voting)**: 개별 분류기의 클래스 예측을 모아 가장 많이 선택된 클래스를 예측하는 방법\n",
    "    - **간접 투표(Soft Voting)**: 개별 분류기의 확률 예측을 평균내어 확률이 가장 높은 클래스를 예측하는 방법\n",
    "\n",
    "<img src=\"02.png\" width=70% height=70%>  \n",
    "(왼쪽: Hard Voting, 오른쪽: Soft Voting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uf8TkMhN00TZ"
   },
   "source": [
    "- **큰 수의 법칙**과 유사한 원리를 사용한다. 큰 수의 법칙은 각 사건이 독립이라는 가정이 있다.\n",
    "- 마찬가지로 앙상블 방법은 예측기가 가능한 한 **서로 독립적**일 때 최고의 성능을 발휘한다. 이를 위해서 각 예측기는 <font color=\"red\">**서로 다른 알고리즘**</font>으로 학습시키는 것이 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R85m130T1bxO"
   },
   "source": [
    "#### 3.1.2.2. 배깅(Bagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfST3TPD1fyb"
   },
   "source": [
    "- **배깅(Bagging)**: 하나의 알고리즘을 사용하고 훈련 세트의 서브셋을 **중복을 허용하여** 무작위로 구성하여 분류기를 각기 다르게 학습시키는 방법\n",
    "- **페이스팅(Pasting)**: 배깅과 유사하지만 훈련 세트의 서브셋을 **중복을 허용하지 않고** 무작위로 구성하는 방법\n",
    "\n",
    "<img src=\"03.png\" width=70% height=70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxFAPwjM6Al5"
   },
   "source": [
    "- 위와 같은 방법으로 학습이 잘 되는 대표적인 이유는 **다양성 확보**이다. 서로 다른 데이터로 학습하면 각 분류기는 서로 다른 특징과 패턴을 파악하게 되는 것이다.\n",
    "- 일반적으로 배깅이 페이스팅보다 더 나은 모델을 만든다. 중복을 허용하기 때문에 더 많은 부분집합이 사용되어 다양성이 증가하기 때문이라고 예상된다.\n",
    "- 최종 결과는 **통계적 최빈값**, 즉 **직접 투표**와 같은 방식으로 예측한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fh798a6E5Y3b"
   },
   "source": [
    "#### 3.1.2.3. 부스팅(Boosting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjpFzv9q59VX"
   },
   "source": [
    "- **부스팅(Boosting)**: 약한 학습기를 여러 개 순차적으로 연결하여 강한 학습기를 만드는 앙상블 방법.\n",
    "<img src=\"04.png\" width=70% height=70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YeSR8EWO-2Ra"
   },
   "source": [
    "- **에이다부스트(AdaBoost)**: 이전 분류기에서 <u>잘못 예측한 훈련 샘플의 가중치를 상대적으로 높여</u> 다음 분류기가 예측하도록 하는 방식\n",
    "    - 가중치는 다음과 같이 설정한다.\n",
    "    1. 먼저 에러율을 구한다.\n",
    "$$\n",
    "r_j=\\frac{\\sum_{i=1, \\, y_{j}^{(i)}=t^{(i)}}^{m}w^{(i)}}{\\sum_{i=1}^{m}w^{(i)}}\n",
    "$$\n",
    "($y_{j}^{(i)}$: $i$번째 샘플에 대한 $j$번째 예측기의 예측, $t^{(i)}$: $i$번째 샘플의 정답, $w^{(i)}$: $i$번째 샘플의 가중치)  \n",
    "(정리하면, 분자는 정답을 맞힌 샘플들의 가중치의 합이고, 분모는 모든 샘플들의 가중치의 합이다.)\n",
    "\n",
    "    2. 예측기 가중치를 구한다. 이 값은 예측기가 정확할수록 가중치가 높아진다.(에러율이 0.5이면 0, 0.5보다 높으면 양수, 0.5보다 낮으면 음수)\n",
    "$$\n",
    "\\alpha_{j}=\\eta \\log \\frac{1-r_{j}}{r_{j}}\n",
    "$$\n",
    "($\\eta$: 학습률(기본값: 1))\n",
    "    3. 가중치를 다음과 같이 업데이트한다.\n",
    "$$\n",
    "w^{(i)} = \\left\\{ \\begin{array}{ll}\n",
    "w^{(i)}, & y_{j}^{(i)}=t^{(i)}\\\\\n",
    "w^{(i)} \\exp(\\alpha_{j}), & y_{j}^{(i)} \\neq t^{(i)}\n",
    "\\end{array} \\right.\n",
    "$$\n",
    "    4. 모든 샘플의 가중치를 정규화한다. (가중치를 모든 가중치의 합으로 나눈다.)\n",
    "    5. 예측기의 학습 과정을 시행한다. 이때 비용 함수(손실 함수)에는 가중치를 반영한다.\n",
    "    6. 1~5를 반복한다.\n",
    "    7. 최종 예측은 다음과 같은 식으로 구한다.\n",
    "$$\n",
    "y(x) = \\arg\\max_{k} \\sum_{j=1, \\, y_j(x)=k}^{N}\\alpha_{j}\n",
    "$$\n",
    "($N$: 예측기의 개수)  \n",
    "(즉, 각 클래스의 예측값을 예측기 가중치의 합으로 구하고, 그 중 가장 가중치의 합이 큰 것을 예측 클래스로 택한다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFie8_1H_Pnj"
   },
   "source": [
    "- **그레이디언트 부스팅(Gradient Boosting)**: 에이다부스트와는 달리 가중치를 수정하지 않고, 이전까지의 오차를 보정하도록 이전 예측기가 만든 **잔여 오차(residual error)**에 새로운 예측기를 학습시킨다.\n",
    "    - **잔여 오차(Residual Error)**: 정답과 예측의 차이"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCMr_TutQ-3R"
   },
   "source": [
    "#### 3.1.2.4. 스태킹(Stacking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JA0PifEbRBfS"
   },
   "source": [
    "- **스태킹(Stacking)**: 앙상블에 속한 모든 예측기의 예측을 또 다른 마지막 예측기가 취합하여 최종 예측을 만드는 앙상블 기법\n",
    "    - 이때 마지막 예측기를 **블랜더(Blender)** 또는 **메타 학습기(Meta Learner)**라 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cdYv3YYYQtxj"
   },
   "source": [
    "### 3.1.3. 랜덤 포레스트와 XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iHEqjI8SAmq"
   },
   "source": [
    "- **랜덤 포레스트(Random Forest)**: **배깅** 방법을 적용한 **결정 트리**의 앙상블\n",
    "    - 장점: 1) 배깅을 활용하여 다양성을 높임. 2) **특성 중요도**를 측정하기 쉽다.\n",
    "    - **특성 중요도**: 어떤 특성을 사용한 노드가 평균적으로 불순도를 감소시키는 정도이며, (현재 노드의 샘플 비율 x 불순도) - (왼쪽 자식 노드의 샘플 비율 x 불순도) - (오른쪽 자식 노드의 샘플 비율 x 불순도)로 계산된다.\n",
    "        - 결정 트리는 모두 특성 중요도를 계산할 수 있지만 트리의 특성 상 일부 특성이 무시될 수도 있다. 랜덤 포레스트는 다양성으로 인해 거의 모든 특성에 대한 중요도를 계산할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1t8_Z98Uxvp"
   },
   "source": [
    "- **XGBoost**: 익스트림 그레이디언트 부스팅(Extreme Gradient Boosting)의 약자로, 그레이디언트 부스팅을 변형하여 높은 성능을 보여주는 기능을 가진 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iPlFByVKYfNq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jy8-9FtDbAKh"
   },
   "source": [
    "## 3.2. Pytorch를 활용한 DCGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8_lCIdy-x1iq"
   },
   "source": [
    "# 참고자료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHPWPGvnx1iq"
   },
   "source": [
    "1. 투빅스 20기 정규세션 1주차 Frameworks 강의 자료: https://drive.google.com/file/d/1BbkRHvRvJ41iPhZvCV8sA7TrBAUb-s2P/view\n",
    "2. Is PyTorch a framework or a library?, Quora: https://www.quora.com/Is-PyTorch-a-framework-or-a-library\n",
    "3. Pytorch, 위키백과(영문): https://en.wikipedia.org/wiki/PyTorch\n",
    "4. Pytorch, 위키백과(한글): https://ko.wikipedia.org/wiki/PyTorch\n",
    "5. 딥러닝 파이토치 교과서, 길벗\n",
    "6. 딥러닝 Express, 생능출판\n",
    "7. 핸즈온 머신러닝 2판, 한빛미디어\n",
    "8. 결정 트리 학습법, 위키백과, https://ko.wikipedia.org/wiki/%EA%B2%B0%EC%A0%95_%ED%8A%B8%EB%A6%AC_%ED%95%99%EC%8A%B5%EB%B2%95\n",
    "9. 결정 트리 시각화 그림 출처: https://ko.wikipedia.org/wiki/%EA%B2%B0%EC%A0%95_%ED%8A%B8%EB%A6%AC_%ED%95%99%EC%8A%B5%EB%B2%95#/media/%ED%8C%8C%EC%9D%BC:CART_tree_titanic_survivors_KOR.png\n",
    "10. Bootstrap Aggregating(Bagging), 위키백과: https://en.wikipedia.org/wiki/Bootstrap_aggregating\n",
    "11. 배깅 그림 출처: https://en.wikipedia.org/wiki/Bootstrap_aggregating#/media/File:Ensemble_Bagging.svg\n",
    "12. Boosting, 위키백과: https://en.wikipedia.org/wiki/Boosting_(machine_learning)\n",
    "13. 부스팅 그림 출처: https://en.wikipedia.org/wiki/Boosting_(machine_learning)#/media/File:Ensemble_Boosting.svg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xRS7q095Yerr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
