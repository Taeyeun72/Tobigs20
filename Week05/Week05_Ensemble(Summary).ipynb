{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFSBJCf_z2k2"
   },
   "source": [
    "# Tobig's 20기 5주차 정규세션 Ensemble 과제(1): 강의 내용 요약"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFlx-ypW0Sih"
   },
   "source": [
    "- 제출자: 20기 황태연  \n",
    "- 제출 일자: 2023.08.22. (화)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "khynqp0EENPq"
   },
   "source": [
    "- **앙상블(Ensemble)**: 일련의 모델들을 조합하여 더 좋은 예측을 하는 모델을 만드는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adXFl6Oo1Mzm"
   },
   "source": [
    "# 1. Bias & Variance Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9y2Qu8Nv2D5q"
   },
   "source": [
    "- 앙상블에서의 편향과 분산은 무엇을 의미하는지 알아보자.\n",
    "    - **편향(Bias)**: 모델의 평균과 정답과의 차이\n",
    "    - **분산(Variance)**: 모델의 평균과 개별 모델의 차이"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4-f3pjYJoRf"
   },
   "source": [
    "- 앙상블에서의 개별 모델의 예측은 편향과 분산으로 **분해(Decomposition)**할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvcGyPww2D7v"
   },
   "source": [
    "- $F^*$: 최종적으로 학습하고자 하는 모델의 함수\n",
    "- $\\epsilon$: 자연발생하는 에러($\\epsilon \\sim N(0, \\sigma^2)$)\n",
    "- $y$: 정답($y=F^*(x)+\\epsilon$)\n",
    "- $\\hat{F}_i$: 예측 모델의 함수\n",
    "- $\\bar{F}$: 예측 모델의 평균($\\bar{F}(x) = E[\\hat{F}_i(x)]$) 이라 하자.\n",
    "- 개별 모델이 새로운 데이터 $x_0$에 대해 예측할 때의 오차 MSE를 편향과 분산으로 분해하는 과정은 다음과 같다.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    Error(x_0) &= E[(y-\\hat{F}(x_0))^2] \\\\\n",
    "    &= E[(F^*(x_0)+\\epsilon-\\hat{F}(x_0))^2] \\\\\n",
    "    &= E[(F^*(x_0)-\\hat{F}(x_0))^2 + 2(F^*(x_0)-\\hat{F}(x_0))\\epsilon + \\epsilon^2] \\\\\n",
    "    &= E[(F^*(x_0)-\\hat{F}(x_0))^2] + 2E[(F^*(x_0)-\\hat{F}(x_0))]E[\\epsilon] + E[\\epsilon^2] \\\\\n",
    "    &= E[(F^*(x_0)-\\hat{F}(x_0))^2] + \\sigma^2 \\quad (\\because E[\\epsilon] = 0, E[\\epsilon^2] = V[\\epsilon] - E[\\epsilon]^2 = \\sigma^2)\\\\\n",
    "    &= E[(F^*(x_0) - \\bar{F}(x_0) + \\bar{F}(x_0) - \\hat{F}(x_0))^2] + \\sigma^2 \\\\\n",
    "    &= E[(F^*(x_0) - \\bar{F}(x_0))^2] + E[(\\bar{F}(x_0) - \\hat{F}(x_0))^2] + \\sigma^2 \\\\\n",
    "    &= (F^*(x_0) - \\bar{F}(x_0))^2 + E[(\\bar{F}(x_0) - \\hat{F}(x_0))^2] + \\sigma^2 \\quad (\\because F^*(x_0), \\bar{F}(x_0) \\textrm{are constants})\\\\\n",
    "    &= Bias^2(\\hat{F}(x_0)) + Var(\\hat{F}(x_0)) + \\sigma^2\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1drA3jcY2D97"
   },
   "source": [
    "- 편향(Bias)은 모델의 예측이 정답에 얼마나 가까운지를 의미한다.\n",
    "- 분산(Vias)은 모델의 예측이 얼마나 잘 모여있는지를 의미한다.\n",
    "- 따라서 편향과 분산이 작을수록 좋은 모델을 의미한다.\n",
    "- 그림으로 정리하면 다음과 같다.\n",
    "\n",
    "<img src=\"01.png\" width=70% height=70%>  \n",
    "\n",
    "(그림 출처: https://imgur.com/DxsRMnO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wr7vGO547BfK"
   },
   "source": [
    "- **편향과 분산에 따른 모델의 복잡도**\n",
    "    - 위 그림에서 2번째에 해당하는 [편향은 낮고 분산이 높은 모델]은 대체적으로 **복잡도가 높은 모델**인 경향이 있다.\n",
    "        - 편향이 낮은 이유: 복잡한만큼 정확하게 예측할 가능성이 높기 때문이다.\n",
    "        - 분산이 높은 이유: 학습 과정에서 노이즈까지 학습했을 가능성이 높기 때문이다.\n",
    "        - 예: Dicision Tree(결정 트리), ANN(Artificial Neural Networks, 인공신경망), SVM(Support Vector Machine, 서포트 벡터 머신), k값이 작은 KNN(K-Nearest Neighbors, k-최근접 이웃)\n",
    "    - 위 그림에서 3번째에 해당하는 [편향은 높고 분산은 낮은 모델]은 대체적으로 **복잡도가 낮은 모델**인 경향이 있다.\n",
    "        - 편향이 높은 이유: 복잡도가 낮아서 정확하게 예측하기 어렵기 때문이다.\n",
    "        - 분산이 낮은 이유: 학습 과정에서 데이터의 정보를 충분히 반영하지 못해서 데이터의 변화에 민감하게 반응하지 못할 수 있기 때문이다.\n",
    "        - 예: Logistic Regression(로지스틱 회귀), LDA(Linear Discriminant Analysis, 선형 판별 분석), k값이 큰 KNN(K-Nearest Neighbors, k-최근접 이웃)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxRedQ702EAd"
   },
   "source": [
    "- 앙상블은 편향 또는 분산을 감소시키는 역할을 한다.\n",
    "    - 편향을 감소시키는 앙상블(위 그림에서 2번째 -> 4번째) 예시: Bagging, Random Forest\n",
    "    - 편차를 감소시키는 앙상블(위 그림에서 3번째 -> 4번째) 예시: AdaBoost, XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gv3msJnO2ECY"
   },
   "source": [
    "# 2. Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKs0J2TR2EEd"
   },
   "source": [
    "- **배깅(Bagging)**: 하나의 알고리즘을 사용하고 훈련 세트의 서브셋을 **중복을 허용하여** 무작위로 구성하여 분류기를 각기 다르게 학습시키는 방법\n",
    "    - 이때 서브셋을 **bootstrap**이라 부른다.\n",
    "    - 이와 같은 방법으로 학습이 잘 되는 대표적인 이유는 **다양성 확보**이다. 서로 다른 데이터로 학습하면 각 분류기는 서로 다른 특징과 패턴을 파악하게 되는 것이다.\n",
    "\n",
    "<img src=\"02.png\" width=70% height=70%>  \n",
    "(출처: https://en.wikipedia.org/wiki/Bootstrap_aggregating#/media/File:Ensemble_Bagging.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JlpNLsV02EGZ"
   },
   "source": [
    "**1. Bootstrapping**\n",
    "- 배깅에서는 복원추출로 bootstrap을 구성하므로, 모든 bootstrap에 포함되지 않는 데이터가 약 37% 발생한다.\n",
    "- 이렇게 bootstrap에 포함되지 않는 데이터의 샘플을 **OOB(Out of Bag) 샘플**이라 한다.\n",
    "> - OOB 샘플이 전체 데이터의 약 37%인 이유는 다음과 같다.\n",
    "> - 어떤 데이터가 $N$개의 bootstrap 중 임의로 선택한 하나의 bootstrap에 포함되지 않을 확률은 $1-\\frac{1}{N}$이다.\n",
    "> - $N$개의 bootstrap에 대해 이를 반복하면 어떤 데이터가 $N$개의 bootstrap에 속하지 않을 확률은 $(1-\\frac{1}{N})^N$이다.\n",
    "> - $N$이 무한히 커지면 다음과 같다.\n",
    "> $$ \\lim_{N \\rightarrow \\infty} \\Big(1-\\frac{1}{N}\\Big)^N = \\lim_{N \\rightarrow \\infty} \\Big(\\frac{N-1}{N}\\Big)^N = \\lim_{N \\rightarrow \\infty} \\Big(\\frac{1}{(1-\\frac{1}{N-1})^N}\\Big) = e^{-1} \\approx 0.368 $$\n",
    "\n",
    "- OOB 샘플을 **검증 데이터로 활용**한다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nxk3hz5a2EJj"
   },
   "source": [
    "**2. Aggregating**\n",
    "- **Aggregating**: 여러 모델로부터의 예측을 모아서 최종 예측을 결정하는 과정\n",
    "- Aggregating에는 다음과 같은 방법들이 있다.\n",
    "    1. **Majority voting(Hard voting)**: 개별 분류기의 클래스 예측을 모아 가장 많이 선택된 클래스를 예측하는 방법\n",
    "    2. **Weighted voting(Soft voting)**: 개별 분류기의 확률 예측을 평균내어 확률이 가장 높은 클래스를 예측하는 방법\n",
    "    3. **Stacking**: 앙상블에 속한 모든 예측기의 예측을 또 다른 **마지막 예측기가 취합**하여 최종 예측을 만드는 앙상블 기법\n",
    "        - 이때 마지막 예측기를 **블랜더(Blender)** 또는 **메타 학습기(Meta Learner)**라 한다.\n",
    "\n",
    "<img src=\"03.png\" width=70% height=70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zhO3IBAD2ELu"
   },
   "source": [
    "# 3. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBHTaapG2ENi"
   },
   "source": [
    "- **랜덤 포레스트(Random Forest)**: **배깅** 중 한 종류로, **결정 트리(Decision Tree)**를 기반으로 하는 앙상블 모델\n",
    "    - **결정 트리(Decision Tree)**: 트리 구조를 사용하여 분류와 회귀 작업을 수행하는 머신러닝 알고리즘\n",
    "    - 랜덤 포레스트는 다음과 같은 특징이 있다.\n",
    "        1. 배깅 사용: 다양성을 높여 성능을 향상시킨다.\n",
    "        2. 변수를 랜덤하게 사용: 편향이 다소 증가할 수 있으나, 과적합이 거의 발생하지 않으며 분산이 감소한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KW4S0bEYexrz"
   },
   "source": [
    "- 랜덤 포레스트를 이용하면 **변수 중요도**를 산출할 수 있다.\n",
    "- **변수 중요도**: 변수가 모델의 예측에 영향을 주는 정도\n",
    "    - 변수 중요도를 계산하는 여러 방법이 있다.\n",
    "    1. **MDI(Mean Decrease in Impurity) Importance**: 어떤 변수을 사용한 노드가 평균적으로 불순도를 감소시키는 정도로 측정하며, (현재 노드의 샘플 비율 x 불순도) - (왼쪽 자식 노드의 샘플 비율 x 불순도) - (오른쪽 자식 노드의 샘플 비율 x 불순도)로 계산된다. 변수 중요도의 합이 1이 되도록 정규화를 하며, 모든 노드의 평균으로 변수 중요도를 구한다.\n",
    "    2. **Permutation Importance**: 원래 데이터셋의 OOB 에러와 **특정 변수를 무작위로 섞은 후**의 OOB 에러의 차이를 중요도로 계산한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5V-jiIXNextn"
   },
   "source": [
    "# 4. AdaBoosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TuIrwkCHqtx"
   },
   "source": [
    "- (참고) 앞으로의 모든 앙상블 기법은 대체로 모두 결정 트리를 기반으로 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CPwJkqzWtaFR"
   },
   "source": [
    "- **부스팅(Boosting)**: 약한 학습기를 여러 개 순차적으로 연결하여 강한 학습기를 만드는 앙상블 방법\n",
    "<img src=\"04.png\" width=70% height=70%>  \n",
    "\n",
    "(출처: https://en.wikipedia.org/wiki/Boosting_(machine_learning)#/media/File:Ensemble_Boosting.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CmmRO9-AoDx"
   },
   "source": [
    "- **배깅과 부스팅의 차이점**\n",
    "    1. 배깅은 병렬적으로 처리하는 반면, 부스팅은 순차적으로 처리한다.\n",
    "    2. 배깅은 복원추출을 통해 무작위로 데이터를 샘플링하여 학습하지 않는 데이터가 생기지만, 부스팅은 학습 데이터를 전부 활용하여 학습한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUTmTn42exvk"
   },
   "source": [
    "- **에이다부스트(AdaBoost)**: 이전 분류기에서 <u>잘못 예측한 훈련 샘플의 가중치를 상대적으로 높여</u> 다음 분류기가 예측하도록 하는 방식\n",
    "    - 가중치는 다음과 같이 설정한다.\n",
    "    1. 먼저 에러율을 구한다.\n",
    "$$\n",
    "r_j=\\frac{\\sum_{i=1, \\, y_{j}^{(i)}=t^{(i)}}^{m}w^{(i)}}{\\sum_{i=1}^{m}w^{(i)}}\n",
    "$$\n",
    "($y_{j}^{(i)}$: $i$번째 샘플에 대한 $j$번째 예측기의 예측, $t^{(i)}$: $i$번째 샘플의 정답, $w^{(i)}$: $i$번째 샘플의 가중치)  \n",
    "(정리하면, 분자는 정답을 맞힌 샘플들의 가중치의 합이고, 분모는 모든 샘플들의 가중치의 합이다.)\n",
    "\n",
    "    2. 예측기 가중치를 구한다. 이 값은 예측기가 정확할수록 가중치가 높아진다.(에러율이 0.5이면 0, 0.5보다 높으면 양수, 0.5보다 낮으면 음수)\n",
    "$$\n",
    "\\alpha_{j}=\\eta \\log \\frac{1-r_{j}}{r_{j}}\n",
    "$$\n",
    "($\\eta$: 학습률(기본값: 1))\n",
    "    3. 가중치를 다음과 같이 업데이트한다.\n",
    "$$\n",
    "w^{(i)} = \\left\\{ \\begin{array}{ll}\n",
    "w^{(i)}, & y_{j}^{(i)}=t^{(i)}\\\\\n",
    "w^{(i)} \\exp(\\alpha_{j}), & y_{j}^{(i)} \\neq t^{(i)}\n",
    "\\end{array} \\right.\n",
    "$$\n",
    "    4. 모든 샘플의 가중치를 정규화한다. (가중치를 모든 가중치의 합으로 나눈다.)\n",
    "    5. 예측기의 학습 과정을 시행한다. 이때 비용 함수(손실 함수)에는 가중치를 반영한다.\n",
    "    6. 1~5를 반복한다.\n",
    "    7. 최종 예측은 다음과 같은 식으로 구한다.\n",
    "$$\n",
    "y(x) = \\arg\\max_{k} \\sum_{j=1, \\, y_j(x)=k}^{N}\\alpha_{j}\n",
    "$$\n",
    "($N$: 예측기의 개수)  \n",
    "(즉, 각 클래스의 예측값을 예측기 가중치의 합으로 구하고, 그 중 가장 가중치의 합이 큰 것을 예측 클래스로 택한다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gyjGfQlKexxf"
   },
   "source": [
    "# 5. Gradient Boosting Machine(GBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLF2h9WUex0i"
   },
   "source": [
    "- **그레이디언트 부스팅(Gradient Boosting)**: 에이다부스트와는 달리 가중치를 수정하지 않고, 이전까지의 오차를 보정하도록 이전 예측기가 만든 **잔여 오차(residual error)**에 새로운 예측기를 학습시킨다.\n",
    "    - **잔여 오차(Residual Error, 잔차)**: 정답과 예측의 차이\n",
    "$$\n",
    "Residual Error = y - f(x)\n",
    "$$\n",
    "($y$: 정답, $f(x)$: 모델의 예측)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tb57h-aFex3N"
   },
   "source": [
    "- 그래디언트 부스팅 머신(Gradient Boosting Machine)은 어떤 데이터 $x$에 대하여 순차적으로 다음 값을 정답으로 두고 학습한다.\n",
    "$$\n",
    "y \\\\\n",
    "y - f_1(x) \\\\\n",
    "y - f_1(x) - f_2(x) \\\\\n",
    "\\vdots\n",
    "$$\n",
    "($y$: 정답, $f_i$: $i$번째 예측기의 예측 함수)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "btlFqeVnex9n"
   },
   "source": [
    "- 이것이 Gradient랑 관련이 있는 이유는 <u>MSE의 Negative Gradient가 잔차</u>와 같기 때문이다! (Negative Gradient는 손실함수의 값을 감소시키는 방향이다.)\n",
    "$$\n",
    "L = \\frac{1}{2} \\sum_i (y_i - f(x_i))^2 \\\\\n",
    "\\frac{\\partial L}{\\partial f(x_i)} = f(x_i)-y_i \\\\\n",
    "\\therefore Residual Error = y_i - f(x_i) = -\\frac{\\partial L}{\\partial f(x_i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDscy6yrex_E"
   },
   "source": [
    "# 6. XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVzkLyD_eyAx"
   },
   "source": [
    "- **XGBoost(eXtreme Gradient Boost)**: 그레이디언트 부스팅을 변형하여 높은 성능을 보여주는 앙상블 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x46W9lQJeyCP"
   },
   "source": [
    "- XGBoost에 사용되는 핵심적인 2가지 알고리즘\n",
    "    1. **Approximate Algorithm for Split Finding**: 결정 트리의 어떤 노드에 있는 데이터를 몇 개의 **Bucket**으로 나눈 뒤, 각 Bucket에서 가장 최적의 split point를 찾아내는 알고리즘\n",
    "        - 기존의 Basic exact greedy algorithm은 Bucket 없이 모든 데이터에 대해 가장 최적의 split point를 찾아내는 방식이다.\n",
    "        - Basic exact greedy algorithm은 항상 최적의 분할을 찾을 수 있으나 메모리에 모든 데이터가 로드되지 않으면 실행이 불가능하며, 분산 처리가 불가능하다.\n",
    "        - 반면 Approximate Algorithm for Split Finding은 대부분 최적의 분할을 찾진 못하지만 다양성을 높일 수 있고, Bucket별로 **분산 처리가 가능**하여 각 스레드의 메모리 할당량이 적고 시간이 적게 걸린다.\n",
    "    2. **Sparsity-aware Split Finding**: 결측값이거나 One-hot-Encoding으로 인해 데이터의 어떤 값(1 또는 Nan 등)이 희소하게(Sparsely) 존재하는 경우 해당 값을 갖는 데이터는 모두 한쪽으로 split되도록 하는 알고리즘\n",
    "        - **희소한(Sparse) 데이터**를 모두 왼쪽 노드로 보낼 때와 오른쪽으로 보낼 때를 비교해서 최적인 split point를 찾는다. 그리고 둘 중 비교해서 그 중 최적인 split point를 선택한다.\n",
    "        - 장점: 1) 결측값도 데이터로써 다룰 수 있으며 2) 알고리즘 처리 속도가 매우 빠르다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9g5hsFu2ERC"
   },
   "source": [
    "- XGBoost는 상당히 높은 성능을 내어서 실생활의 문제를 해결하는 데에 여러 방면으로 활용되고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bd4CEcJ82EWp"
   },
   "source": [
    "### **참고 자료**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8Polb1oP1vJ"
   },
   "source": [
    "1. 투빅스 20기 5주차 정규세션 Ensemble 강의 자료: https://drive.google.com/file/d/1wyj4OCfYiMk2ORVbKz6de_z3YxQ1oIpR/view?usp=drive_link\n",
    "2. Bias-Variance Decomposition, github: https://ratsgo.github.io/machine%20learning/2017/05/19/biasvar/\n",
    "3. 핸즈온 머신러닝 2판, 한빛미디어\n",
    "4. 편향-분산 트레이드오프, 위키백과: https://ko.wikipedia.org/wiki/%ED%8E%B8%ED%96%A5-%EB%B6%84%EC%82%B0_%ED%8A%B8%EB%A0%88%EC%9D%B4%EB%93%9C%EC%98%A4%ED%94%84\n",
    "5. 04-2: Ensemble Learning - Bias-Variance Decomposition (앙상블 - 편향과 분산), Youtube: https://www.youtube.com/watch?v=mZwszY3kQBg\n",
    "6. 배깅 그림 출처: https://en.wikipedia.org/wiki/Bootstrap_aggregating#/media/File:Ensemble_Bagging.svg\n",
    "7. 랜덤 포레스트에서의 변수 중요도(Variable Importance) 3가지: https://velog.io/@vvakki_/%EB%9E%9C%EB%8D%A4-%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8%EC%97%90%EC%84%9C%EC%9D%98-%EB%B3%80%EC%88%98-%EC%A4%91%EC%9A%94%EB%8F%84Variable-Importance-3%EA%B0%80%EC%A7%80\n",
    "8. 부스팅 그림 출처: https://en.wikipedia.org/wiki/Boosting_(machine_learning)#/media/File:Ensemble_Boosting.svg\n",
    "9. 04-7: Ensemble Learning - XGBoost (앙상블 기법 - XGBoost), Youtube: https://www.youtube.com/watch?v=VHky3d_qZ_E&t=1614s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Ltftta8QdxL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyON+prBzZ4iOJvPm7sqOBGd",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
